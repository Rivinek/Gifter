{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gensim\n",
    "from gifter.modeling.data import lemmatized_frame\n",
    "from gifter.modeling.tokenizer import lemmatize\n",
    "from gensim import corpora\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(texts=[\"default\"], num=2, passes=100):\n",
    "    if (texts == [\"default\"]):\n",
    "        df = lemmatized_frame(\"../data.json\", with_tags=False)\n",
    "        texts = [df['lemmas'].irow(i) for i in range(df.shape[0])]\n",
    "    else:\n",
    "        texts2 = []\n",
    "        for d in texts:\n",
    "            texts2 = texts2 + [lemmatize(d, with_tags=False)]\n",
    "            texts = texts2\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #print \"corpus: \" + str(corpus)\n",
    "    corpora.MmCorpus.serialize('./corpus.mm', corpus)\n",
    "    mm_corpus = corpora.MmCorpus('./corpus.mm')\n",
    "    #print \"mm_corpus: \" + str(mm_corpus)\n",
    "    id2word = {}\n",
    "    for word in dictionary.token2id:\n",
    "        id2word[dictionary.token2id[word]] = word\n",
    "    #lda = gensim.models.ldamodel.LdaModel(\n",
    "    lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "        corpus=mm_corpus,\n",
    "        num_topics=num,\n",
    "        id2word=id2word,\n",
    "        #update_every=1,\n",
    "        eval_every=1,\n",
    "        passes=passes,\n",
    "        workers=1\n",
    "    )\n",
    "    for i in range(0, lda.num_topics):\n",
    "        print \"Topic number \" + str(i) + \" consists of words : \" + lda.print_topic(i)\n",
    "    return lda, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consist of words : 0.045*president + 0.042*obama + 0.028*climate + 0.019*act + 0.015*change + 0.010*day + 0.009*american + 0.009*time + 0.008*http + 0.008*obamacare\n",
      "Topic number 1 consist of words : 0.064*president + 0.053*obama + 0.015*selma + 0.015*work + 0.014*college + 0.013*watch + 0.010*march + 0.010*job + 0.010*year + 0.009*talk\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Apple is releasing a new product\",\n",
    "    \"I love apple\",\n",
    "    \"Amazon sells many things\",\n",
    "    \"Microsoft announces Nokia acquisition\",\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\"\n",
    "]\n",
    "\n",
    "D = [\n",
    "    \"I like to eat broccoli and bananas.\",\n",
    "    \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "    \"Chinchillas and kittens are cute.\",\n",
    "    \"My sister adopted a kitten yesterday.\",\n",
    "    \"Look at this cute hamster munching on a piece of broccoli.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consist of words : 0.166*kitten + 0.100*cute + 0.099*yesterday + 0.099*sister + 0.099*adopt + 0.099*chinchilla + 0.034*broccoli + 0.034*eat + 0.034*banana + 0.034*piece\n",
      "Topic number 1 consist of words : 0.114*eat + 0.114*banana + 0.114*broccoli + 0.068*spinach + 0.068*smoothie + 0.068*breakfast + 0.068*piece + 0.068*munch + 0.068*look + 0.068*hamster\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA(texts=D, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consist of words : 0.075*user + 0.056*time + 0.056*response + 0.056*apple + 0.035*survey + 0.033*relation + 0.033*error + 0.033*measurement + 0.033*perceive + 0.033*opinion\n",
      "Topic number 1 consist of words : 0.060*tree + 0.060*graph + 0.043*human + 0.043*minor + 0.042*interface + 0.026*width + 0.026*order + 0.026*iv + 0.026*quasi + 0.026*machine\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA(texts=documents, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consist of words : 0.130*tell + 0.116*thing + 0.084*turn + 0.069*woman + 0.029*job + 0.029*field + 0.025*line + 0.024*bit + 0.022*air + 0.021*clear\n",
      "Topic number 1 consist of words : 0.099*mr + 0.074*night + 0.062*live + 0.032*rest + 0.025*early + 0.017*coffee + 0.017*force + 0.016*sick + 0.014*suit + 0.013*studio\n",
      "Topic number 2 consist of words : 0.032*change + 0.022*hate + 0.016*lose + 0.014*eat + 0.013*freddy + 0.012*layer + 0.012*blanche + 0.011*subject + 0.010*pet + 0.010*doll\n",
      "Topic number 3 consist of words : 0.209*man + 0.055*miss + 0.043*smile + 0.035*book + 0.033*minute + 0.026*cousin + 0.023*finger + 0.022*pretty + 0.020*ready + 0.016*paint\n",
      "Topic number 4 consist of words : 0.218*look + 0.126*way + 0.078*sit + 0.035*half + 0.032*catch + 0.031*meet + 0.020*heavy + 0.019*church + 0.017*write + 0.016*clearly\n",
      "Topic number 5 consist of words : 0.223*know + 0.067*place + 0.062*open + 0.054*stand + 0.042*set + 0.041*grow + 0.032*speak + 0.026*realize + 0.024*touch + 0.019*trip\n",
      "Topic number 6 consist of words : 0.125*thought + 0.075*away + 0.059*moment + 0.036*lot + 0.032*carry + 0.028*comedy + 0.022*baby + 0.020*moved + 0.017*wake + 0.016*hurry\n",
      "Topic number 7 consist of words : 0.116*person + 0.081*girl + 0.051*hold + 0.032*hadn + 0.031*wear + 0.023*glass + 0.022*best + 0.018*married + 0.017*box + 0.015*plan\n",
      "Topic number 8 consist of words : 0.100*old + 0.067*begin + 0.046*watch + 0.039*white + 0.037*mike + 0.024*able + 0.022*stone + 0.021*stick + 0.021*like + 0.020*city\n",
      "Topic number 9 consist of words : 0.099*good + 0.093*room + 0.036*school + 0.035*follow + 0.018*humor + 0.017*sir + 0.016*sleep + 0.014*painting + 0.013*haven + 0.012*save\n",
      "Topic number 10 consist of words : 0.078*hear + 0.036*couple + 0.030*suddenly + 0.025*question + 0.023*guy + 0.022*warm + 0.018*refuse + 0.016*dance + 0.014*impossible + 0.013*species\n",
      "Topic number 11 consist of words : 0.083*let + 0.039*cold + 0.025*arm + 0.018*wide + 0.017*green + 0.017*spend + 0.016*mad + 0.013*advertising + 0.012*jim + 0.011*safe\n",
      "Topic number 12 consist of words : 0.076*life + 0.056*foot + 0.044*remember + 0.031*beautiful + 0.031*lip + 0.025*meeting + 0.025*rise + 0.023*bad + 0.023*happened + 0.020*reason\n",
      "Topic number 13 consist of words : 0.182*time + 0.093*ask + 0.073*work + 0.058*talk + 0.037*end + 0.037*sound + 0.036*body + 0.023*anne + 0.023*kitchen + 0.019*lady\n",
      "Topic number 14 consist of words : 0.055*voice + 0.042*sure + 0.037*wait + 0.037*high + 0.028*draw + 0.025*add + 0.023*tractor + 0.021*dinner + 0.019*pull + 0.016*modern\n",
      "Topic number 15 consist of words : 0.085*new + 0.042*yes + 0.041*drink + 0.032*soon + 0.030*stare + 0.025*certain + 0.024*poor + 0.021*attention + 0.019*study + 0.018*today\n",
      "Topic number 16 consist of words : 0.108*ll + 0.078*home + 0.048*short + 0.037*mean + 0.036*use + 0.031*martin + 0.015*nod + 0.013*serve + 0.013*performance + 0.012*crack\n",
      "Topic number 17 consist of words : 0.046*leave + 0.035*answer + 0.030*human + 0.027*party + 0.021*picked + 0.020*slide + 0.019*death + 0.018*clean + 0.017*taste + 0.017*quietly\n",
      "Topic number 18 consist of words : 0.055*love + 0.042*mrs + 0.042*great + 0.041*fall + 0.037*horse + 0.034*color + 0.032*case + 0.021*instead + 0.020*simply + 0.019*kid\n",
      "Topic number 19 consist of words : 0.073*try + 0.065*house + 0.056*kind + 0.044*stop + 0.044*hair + 0.034*family + 0.032*maybe + 0.027*die + 0.025*shoe + 0.024*clothe\n",
      "Topic number 20 consist of words : 0.107*year + 0.100*head + 0.070*couldn + 0.057*run + 0.015*hit + 0.013*free + 0.012*cup + 0.011*statement + 0.011*cheek + 0.010*nervous\n",
      "Topic number 21 consist of words : 0.079*face + 0.039*light + 0.037*read + 0.029*figure + 0.028*red + 0.028*step + 0.024*state + 0.020*group + 0.020*breakfast + 0.014*shut\n",
      "Topic number 22 consist of words : 0.089*mother + 0.065*door + 0.059*wasn + 0.053*play + 0.048*morning + 0.039*need + 0.037*god + 0.034*second + 0.029*dog + 0.027*bed\n",
      "Topic number 23 consist of words : 0.099*hand + 0.054*phil + 0.036*far + 0.034*buy + 0.030*close + 0.030*idea + 0.029*note + 0.028*chance + 0.027*wrong + 0.024*remain\n",
      "Topic number 24 consist of words : 0.141*day + 0.041*laugh + 0.033*age + 0.029*hard + 0.023*fine + 0.023*problem + 0.019*afternoon + 0.018*push + 0.018*murder + 0.014*cool\n",
      "Topic number 25 consist of words : 0.119*think + 0.048*wife + 0.048*water + 0.037*ve + 0.035*william + 0.030*blue + 0.028*probably + 0.018*past + 0.017*tongue + 0.016*forever\n",
      "Topic number 26 consist of words : 0.037*win + 0.036*later + 0.035*american + 0.034*believe + 0.032*nice + 0.029*dress + 0.026*dear + 0.024*trouble + 0.021*burn + 0.019*dream\n",
      "Topic number 27 consist of words : 0.067*child + 0.049*week + 0.043*black + 0.041*better + 0.037*funny + 0.036*snake + 0.027*build + 0.021*story + 0.019*pink + 0.017*raise\n",
      "Topic number 28 consist of words : 0.208*come + 0.067*start + 0.043*bring + 0.024*standing + 0.022*lucy + 0.020*shout + 0.018*corner + 0.015*longer + 0.014*admit + 0.013*student\n",
      "Topic number 29 consist of words : 0.041*word + 0.031*order + 0.028*sun + 0.026*dark + 0.025*leg + 0.025*linda + 0.024*business + 0.024*office + 0.021*sorry + 0.021*late\n",
      "Topic number 30 consist of words : 0.121*feel + 0.047*big + 0.042*course + 0.036*friend + 0.032*sort + 0.028*reach + 0.027*picture + 0.024*street + 0.024*college + 0.021*outside\n",
      "Topic number 31 consist of words : 0.085*right + 0.074*boy + 0.038*son + 0.036*letter + 0.031*english + 0.030*mouth + 0.028*husband + 0.025*company + 0.024*dead + 0.022*club\n",
      "Topic number 32 consist of words : 0.036*father + 0.034*drop + 0.031*pass + 0.026*ball + 0.026*break + 0.023*phone + 0.023*doubt + 0.022*enter + 0.018*lay + 0.017*susan\n",
      "Topic number 33 consist of words : 0.106*long + 0.050*small + 0.047*help + 0.040*general + 0.035*matter + 0.028*george + 0.024*town + 0.020*shot + 0.019*learn + 0.018*straight\n",
      "Topic number 34 consist of words : 0.150*little + 0.112*want + 0.035*throw + 0.032*wonder + 0.031*send + 0.027*care + 0.025*finally + 0.022*rock + 0.020*stage + 0.015*board\n",
      "Topic number 35 consist of words : 0.040*mind + 0.036*car + 0.036*world + 0.035*young + 0.033*walk + 0.033*heart + 0.030*wouldn + 0.028*stay + 0.026*wish + 0.022*hope\n",
      "Topic number 36 consist of words : 0.069*left + 0.043*john + 0.028*visit + 0.026*pale + 0.022*tooth + 0.020*point + 0.019*bright + 0.017*concern + 0.017*appear + 0.016*important\n",
      "Topic number 37 consist of words : 0.127*eye + 0.031*drive + 0.028*lead + 0.027*johnnie + 0.025*true + 0.022*ride + 0.020*large + 0.019*york + 0.019*war + 0.016*type\n",
      "Topic number 38 consist of words : 0.039*hour + 0.034*hot + 0.032*worry + 0.030*isn + 0.030*money + 0.029*doctor + 0.026*evening + 0.021*newspaper + 0.019*bottle + 0.019*search\n",
      "Topic number 39 consist of words : 0.033*return + 0.028*wall + 0.026*slowly + 0.024*shake + 0.023*tool + 0.021*apartment + 0.020*fact + 0.020*fight + 0.020*present + 0.017*captain\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA(texts=brown.words(), num=40, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_topic(new_doc=\"User\", dictionary=dic, lda=l):\n",
    "    if (new_doc == \"User\"):\n",
    "        df = lemmatized_frame(\"../data.json\", with_tags=False)\n",
    "        new_doc = list(\n",
    "            itertools.chain(\n",
    "                *[df['lemmas'].irow(i) for i in range(df.shape[0])]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        new_doc = new_doc.lower().split()\n",
    "    new_vec = dictionary.doc2bow(new_doc)\n",
    "    vec_lda = sorted(lda[new_vec], key=lambda vec: vec[1], reverse=True)\n",
    "    print str(vec_lda)\n",
    "    print \"Top topic is topic number \" + str(\n",
    "        vec_lda[0][0]\n",
    "    ) + \" consists of words : \" + lda.print_topic(vec_lda[0][0])\n",
    "    return vec_lda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(37, 0.10770201168570052), (35, 0.072113714853879601), (18, 0.045270958023020565), (26, 0.043610604948622574), (0, 0.041844140845861279), (13, 0.037362392113134038), (4, 0.033857392688869038), (33, 0.030330232861490027), (2, 0.030270737541640273), (30, 0.030212396755356221), (7, 0.027490654492517745), (8, 0.02540333367637548), (14, 0.024759657787775797), (15, 0.024305337142576752), (24, 0.023825393624853961), (12, 0.023805179718994183), (27, 0.023774074024676489), (1, 0.022649045654128168), (34, 0.022443666504283678), (20, 0.021084373805999604), (5, 0.02064703218059941), (19, 0.019670802197358922), (17, 0.019261455409148837), (21, 0.016494188031877143), (6, 0.015780067487399685), (9, 0.015659095963112598), (11, 0.015601231684766835), (23, 0.015127426585328235), (32, 0.015002909256486622), (39, 0.014669656779502348), (22, 0.01463724232061368), (3, 0.013753453902696472), (38, 0.013734268880740688), (29, 0.013569990849101817), (16, 0.013273449229459064), (31, 0.012801310448500712), (10, 0.011463160788300853), (25, 0.010087375040566256)]\n",
      "Top topic is topic number 37 consist of words : 0.127*eye + 0.031*drive + 0.028*lead + 0.027*johnnie + 0.025*true + 0.022*ride + 0.020*large + 0.019*york + 0.019*war + 0.016*type\n",
      "CPU times: user 2.4 s, sys: 67.6 ms, total: 2.47 s\n",
      "Wall time: 2.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(37, 0.10770201168570052),\n",
       " (35, 0.072113714853879601),\n",
       " (18, 0.045270958023020565),\n",
       " (26, 0.043610604948622574),\n",
       " (0, 0.041844140845861279),\n",
       " (13, 0.037362392113134038),\n",
       " (4, 0.033857392688869038),\n",
       " (33, 0.030330232861490027),\n",
       " (2, 0.030270737541640273),\n",
       " (30, 0.030212396755356221),\n",
       " (7, 0.027490654492517745),\n",
       " (8, 0.02540333367637548),\n",
       " (14, 0.024759657787775797),\n",
       " (15, 0.024305337142576752),\n",
       " (24, 0.023825393624853961),\n",
       " (12, 0.023805179718994183),\n",
       " (27, 0.023774074024676489),\n",
       " (1, 0.022649045654128168),\n",
       " (34, 0.022443666504283678),\n",
       " (20, 0.021084373805999604),\n",
       " (5, 0.02064703218059941),\n",
       " (19, 0.019670802197358922),\n",
       " (17, 0.019261455409148837),\n",
       " (21, 0.016494188031877143),\n",
       " (6, 0.015780067487399685),\n",
       " (9, 0.015659095963112598),\n",
       " (11, 0.015601231684766835),\n",
       " (23, 0.015127426585328235),\n",
       " (32, 0.015002909256486622),\n",
       " (39, 0.014669656779502348),\n",
       " (22, 0.01463724232061368),\n",
       " (3, 0.013753453902696472),\n",
       " (38, 0.013734268880740688),\n",
       " (29, 0.013569990849101817),\n",
       " (16, 0.013273449229459064),\n",
       " (31, 0.012801310448500712),\n",
       " (10, 0.011463160788300853),\n",
       " (25, 0.010087375040566256)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.14991535815791426), (29, 0.14978780243077047), (19, 0.14961452069074083), (4, 0.14953320319171437), (18, 0.1493942791292022), (0, 0.14550483639965719)]\n",
      "Top topic is topic number 1 consist of words : 0.099*mr + 0.074*night + 0.062*live + 0.032*rest + 0.025*early + 0.017*coffee + 0.017*force + 0.016*sick + 0.014*suit + 0.013*studio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.14991535815791426),\n",
       " (29, 0.14978780243077047),\n",
       " (19, 0.14961452069074083),\n",
       " (4, 0.14953320319171437),\n",
       " (18, 0.1493942791292022),\n",
       " (0, 0.14550483639965719)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topic(new_doc = \"I love watching films with my family and friends. I will write review of the film we watched yesterday\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
