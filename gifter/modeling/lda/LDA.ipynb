{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gensim\n",
    "from gifter.modeling.data import lemmatized_frame\n",
    "from gifter.modeling.tokenizer import lemmatize\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(texts=\"../data/1/data.json\",documents_list=[],num=2, passes=100,save_model_as='lda.model', save_dic_as='dictionary.dic'):\n",
    "    if not documents_list:\n",
    "        df = lemmatized_frame(open(texts, \"r\"), with_tags=False)\n",
    "        texts = [df['lemmas'].irow(i) for i in range(df.shape[0])]\n",
    "    else:\n",
    "        texts2 = []\n",
    "        for d in documents_list:\n",
    "            texts2 = texts2 + [lemmatize(d, with_tags=False)]\n",
    "            texts = texts2\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #print \"corpus: \" + str(corpus)\n",
    "    corpora.MmCorpus.serialize('./corpus.mm', corpus)\n",
    "    mm_corpus = corpora.MmCorpus('./corpus.mm')\n",
    "    #print \"mm_corpus: \" + str(mm_corpus)\n",
    "    id2word = {}\n",
    "    for word in dictionary.token2id:\n",
    "        id2word[dictionary.token2id[word]] = word\n",
    "    lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "        corpus=mm_corpus,\n",
    "        num_topics=num,\n",
    "        id2word=id2word,\n",
    "        eval_every=1,\n",
    "        passes=passes,\n",
    "        workers=1\n",
    "    )\n",
    "    lda.save(save_model_as)\n",
    "    dictionary.save(save_dic_as)\n",
    "    for i in range(0, lda.num_topics):\n",
    "        print \"Topic number \" + str(i) + \" consists of words : \" + lda.print_topic(i)\n",
    "    return lda, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marta/.virtualenvs/gifter/Gifter/gifter/modeling/tokenizer.py:56: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  pattern = re.compile(\"(#|RT |{})\".format(unidecode(to_remove)), re.I)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consists of words : 0.015*state + 0.012*lead + 0.010*secretary + 0.009*curry + 0.008*minister + 0.007*conservative + 0.007*tonight + 0.007*amp + 0.006*new + 0.006*vote\n",
      "Topic number 1 consists of words : 0.029*fcb + 0.012*win + 0.011*title + 0.010*atleti + 0.009*game + 0.009*atm + 0.009*campion + 0.008*climate + 0.008*president + 0.007*league\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2 = models.LdaModel.load('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic2 = corpora.Dictionary.load('dictionary.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = [\n",
    "    \"I like to eat broccoli and bananas.\",\n",
    "    \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "    \"Chinchillas and kittens are cute.\",\n",
    "    \"My sister adopted a kitten yesterday.\",\n",
    "    \"Look at this cute hamster munching on a piece of broccoli.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consists of words : 0.119*cute + 0.119*kitten + 0.072*broccoli + 0.071*piece + 0.071*munch + 0.071*look + 0.071*hamster + 0.071*yesterday + 0.071*sister + 0.071*adopt\n",
      "Topic number 1 consists of words : 0.155*eat + 0.155*banana + 0.093*spinach + 0.093*smoothie + 0.093*breakfast + 0.093*broccoli + 0.032*chinchilla + 0.032*kitten + 0.032*cute + 0.032*yesterday\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA(documents_list=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0 consist of words : 0.130*tell + 0.116*thing + 0.084*turn + 0.069*woman + 0.029*job + 0.029*field + 0.025*line + 0.024*bit + 0.022*air + 0.021*clear\n",
      "Topic number 1 consist of words : 0.099*mr + 0.074*night + 0.062*live + 0.032*rest + 0.025*early + 0.017*coffee + 0.017*force + 0.016*sick + 0.014*suit + 0.013*studio\n",
      "Topic number 2 consist of words : 0.032*change + 0.022*hate + 0.016*lose + 0.014*eat + 0.013*freddy + 0.012*layer + 0.012*blanche + 0.011*subject + 0.010*pet + 0.010*doll\n",
      "Topic number 3 consist of words : 0.209*man + 0.055*miss + 0.043*smile + 0.035*book + 0.033*minute + 0.026*cousin + 0.023*finger + 0.022*pretty + 0.020*ready + 0.016*paint\n",
      "Topic number 4 consist of words : 0.218*look + 0.126*way + 0.078*sit + 0.035*half + 0.032*catch + 0.031*meet + 0.020*heavy + 0.019*church + 0.017*write + 0.016*clearly\n",
      "Topic number 5 consist of words : 0.223*know + 0.067*place + 0.062*open + 0.054*stand + 0.042*set + 0.041*grow + 0.032*speak + 0.026*realize + 0.024*touch + 0.019*trip\n",
      "Topic number 6 consist of words : 0.125*thought + 0.075*away + 0.059*moment + 0.036*lot + 0.032*carry + 0.028*comedy + 0.022*baby + 0.020*moved + 0.017*wake + 0.016*hurry\n",
      "Topic number 7 consist of words : 0.116*person + 0.081*girl + 0.051*hold + 0.032*hadn + 0.031*wear + 0.023*glass + 0.022*best + 0.018*married + 0.017*box + 0.015*plan\n",
      "Topic number 8 consist of words : 0.100*old + 0.067*begin + 0.046*watch + 0.039*white + 0.037*mike + 0.024*able + 0.022*stone + 0.021*stick + 0.021*like + 0.020*city\n",
      "Topic number 9 consist of words : 0.099*good + 0.093*room + 0.036*school + 0.035*follow + 0.018*humor + 0.017*sir + 0.016*sleep + 0.014*painting + 0.013*haven + 0.012*save\n",
      "Topic number 10 consist of words : 0.078*hear + 0.036*couple + 0.030*suddenly + 0.025*question + 0.023*guy + 0.022*warm + 0.018*refuse + 0.016*dance + 0.014*impossible + 0.013*species\n",
      "Topic number 11 consist of words : 0.083*let + 0.039*cold + 0.025*arm + 0.018*wide + 0.017*green + 0.017*spend + 0.016*mad + 0.013*advertising + 0.012*jim + 0.011*safe\n",
      "Topic number 12 consist of words : 0.076*life + 0.056*foot + 0.044*remember + 0.031*beautiful + 0.031*lip + 0.025*meeting + 0.025*rise + 0.023*bad + 0.023*happened + 0.020*reason\n",
      "Topic number 13 consist of words : 0.182*time + 0.093*ask + 0.073*work + 0.058*talk + 0.037*end + 0.037*sound + 0.036*body + 0.023*anne + 0.023*kitchen + 0.019*lady\n",
      "Topic number 14 consist of words : 0.055*voice + 0.042*sure + 0.037*wait + 0.037*high + 0.028*draw + 0.025*add + 0.023*tractor + 0.021*dinner + 0.019*pull + 0.016*modern\n",
      "Topic number 15 consist of words : 0.085*new + 0.042*yes + 0.041*drink + 0.032*soon + 0.030*stare + 0.025*certain + 0.024*poor + 0.021*attention + 0.019*study + 0.018*today\n",
      "Topic number 16 consist of words : 0.108*ll + 0.078*home + 0.048*short + 0.037*mean + 0.036*use + 0.031*martin + 0.015*nod + 0.013*serve + 0.013*performance + 0.012*crack\n",
      "Topic number 17 consist of words : 0.046*leave + 0.035*answer + 0.030*human + 0.027*party + 0.021*picked + 0.020*slide + 0.019*death + 0.018*clean + 0.017*taste + 0.017*quietly\n",
      "Topic number 18 consist of words : 0.055*love + 0.042*mrs + 0.042*great + 0.041*fall + 0.037*horse + 0.034*color + 0.032*case + 0.021*instead + 0.020*simply + 0.019*kid\n",
      "Topic number 19 consist of words : 0.073*try + 0.065*house + 0.056*kind + 0.044*stop + 0.044*hair + 0.034*family + 0.032*maybe + 0.027*die + 0.025*shoe + 0.024*clothe\n",
      "Topic number 20 consist of words : 0.107*year + 0.100*head + 0.070*couldn + 0.057*run + 0.015*hit + 0.013*free + 0.012*cup + 0.011*statement + 0.011*cheek + 0.010*nervous\n",
      "Topic number 21 consist of words : 0.079*face + 0.039*light + 0.037*read + 0.029*figure + 0.028*red + 0.028*step + 0.024*state + 0.020*group + 0.020*breakfast + 0.014*shut\n",
      "Topic number 22 consist of words : 0.089*mother + 0.065*door + 0.059*wasn + 0.053*play + 0.048*morning + 0.039*need + 0.037*god + 0.034*second + 0.029*dog + 0.027*bed\n",
      "Topic number 23 consist of words : 0.099*hand + 0.054*phil + 0.036*far + 0.034*buy + 0.030*close + 0.030*idea + 0.029*note + 0.028*chance + 0.027*wrong + 0.024*remain\n",
      "Topic number 24 consist of words : 0.141*day + 0.041*laugh + 0.033*age + 0.029*hard + 0.023*fine + 0.023*problem + 0.019*afternoon + 0.018*push + 0.018*murder + 0.014*cool\n",
      "Topic number 25 consist of words : 0.119*think + 0.048*wife + 0.048*water + 0.037*ve + 0.035*william + 0.030*blue + 0.028*probably + 0.018*past + 0.017*tongue + 0.016*forever\n",
      "Topic number 26 consist of words : 0.037*win + 0.036*later + 0.035*american + 0.034*believe + 0.032*nice + 0.029*dress + 0.026*dear + 0.024*trouble + 0.021*burn + 0.019*dream\n",
      "Topic number 27 consist of words : 0.067*child + 0.049*week + 0.043*black + 0.041*better + 0.037*funny + 0.036*snake + 0.027*build + 0.021*story + 0.019*pink + 0.017*raise\n",
      "Topic number 28 consist of words : 0.208*come + 0.067*start + 0.043*bring + 0.024*standing + 0.022*lucy + 0.020*shout + 0.018*corner + 0.015*longer + 0.014*admit + 0.013*student\n",
      "Topic number 29 consist of words : 0.041*word + 0.031*order + 0.028*sun + 0.026*dark + 0.025*leg + 0.025*linda + 0.024*business + 0.024*office + 0.021*sorry + 0.021*late\n",
      "Topic number 30 consist of words : 0.121*feel + 0.047*big + 0.042*course + 0.036*friend + 0.032*sort + 0.028*reach + 0.027*picture + 0.024*street + 0.024*college + 0.021*outside\n",
      "Topic number 31 consist of words : 0.085*right + 0.074*boy + 0.038*son + 0.036*letter + 0.031*english + 0.030*mouth + 0.028*husband + 0.025*company + 0.024*dead + 0.022*club\n",
      "Topic number 32 consist of words : 0.036*father + 0.034*drop + 0.031*pass + 0.026*ball + 0.026*break + 0.023*phone + 0.023*doubt + 0.022*enter + 0.018*lay + 0.017*susan\n",
      "Topic number 33 consist of words : 0.106*long + 0.050*small + 0.047*help + 0.040*general + 0.035*matter + 0.028*george + 0.024*town + 0.020*shot + 0.019*learn + 0.018*straight\n",
      "Topic number 34 consist of words : 0.150*little + 0.112*want + 0.035*throw + 0.032*wonder + 0.031*send + 0.027*care + 0.025*finally + 0.022*rock + 0.020*stage + 0.015*board\n",
      "Topic number 35 consist of words : 0.040*mind + 0.036*car + 0.036*world + 0.035*young + 0.033*walk + 0.033*heart + 0.030*wouldn + 0.028*stay + 0.026*wish + 0.022*hope\n",
      "Topic number 36 consist of words : 0.069*left + 0.043*john + 0.028*visit + 0.026*pale + 0.022*tooth + 0.020*point + 0.019*bright + 0.017*concern + 0.017*appear + 0.016*important\n",
      "Topic number 37 consist of words : 0.127*eye + 0.031*drive + 0.028*lead + 0.027*johnnie + 0.025*true + 0.022*ride + 0.020*large + 0.019*york + 0.019*war + 0.016*type\n",
      "Topic number 38 consist of words : 0.039*hour + 0.034*hot + 0.032*worry + 0.030*isn + 0.030*money + 0.029*doctor + 0.026*evening + 0.021*newspaper + 0.019*bottle + 0.019*search\n",
      "Topic number 39 consist of words : 0.033*return + 0.028*wall + 0.026*slowly + 0.024*shake + 0.023*tool + 0.021*apartment + 0.020*fact + 0.020*fight + 0.020*present + 0.017*captain\n"
     ]
    }
   ],
   "source": [
    "l, dic = LDA(texts=brown.words(), num=40, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_topic(new_doc=\"../data/data.json\",documents_list=[], dictionary=dic, lda=l):\n",
    "    if not documents_list:\n",
    "        df = lemmatized_frame(open(new_doc, \"r\"), with_tags=False)\n",
    "        new_doc = list(\n",
    "            itertools.chain(\n",
    "                *[df['lemmas'].irow(i) for i in range(df.shape[0])]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        new_doc = new_doc.lower().split()\n",
    "    new_vec = dictionary.doc2bow(new_doc)\n",
    "    vec_lda = sorted(lda[new_vec], key=lambda vec: vec[1], reverse=True)\n",
    "    print str(vec_lda)\n",
    "    print \"Top topic is topic number \" + str(\n",
    "        vec_lda[0][0]\n",
    "    ) + \" consists of words : \" + lda.print_topic(vec_lda[0][0])\n",
    "    return vec_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.52099177690025189), (1, 0.47900822309974816)]\n",
      "Top topic is topic number 0 consists of words : 0.015*state + 0.012*lead + 0.010*secretary + 0.009*curry + 0.008*minister + 0.007*conservative + 0.007*tonight + 0.007*amp + 0.006*new + 0.006*vote\n",
      "CPU times: user 1.53 s, sys: 64.2 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.52099177690025189), (1, 0.47900822309974816)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_topic(dictionary=dic2,lda=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5), (1, 0.5)]\n",
      "Top topic is topic number 0 consists of words : 0.119*cute + 0.119*kitten + 0.072*broccoli + 0.071*piece + 0.071*munch + 0.071*look + 0.071*hamster + 0.071*yesterday + 0.071*sister + 0.071*adopt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.5), (1, 0.5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topic(documents_list = \"I love watching films about animals with my family and friends. I will write review of the film we watched yesterday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
